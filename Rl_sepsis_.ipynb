{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wR3tUWISJlP8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWXuSvjCKbGO",
        "outputId": "ec717a81-d8c8-494f-b9dd-372273415cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Démarrage de l'entraînement...\n",
            "Données chargées: (500, 11)\n",
            "Episode 10\tAverage Reward: -0.39\n",
            "Episode 20\tAverage Reward: -0.53\n",
            "Episode 30\tAverage Reward: -0.63\n",
            "Episode 40\tAverage Reward: -0.73\n",
            "Episode 50\tAverage Reward: -0.77\n",
            "Episode 60\tAverage Reward: -0.86\n",
            "Episode 70\tAverage Reward: -0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 80\tAverage Reward: -0.73\n",
            "Episode 90\tAverage Reward: -0.73\n",
            "Episode 100\tAverage Reward: -0.75\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_100.pth\n",
            "Episode 110\tAverage Reward: -0.73\n",
            "Episode 120\tAverage Reward: -0.70\n",
            "Episode 130\tAverage Reward: -0.82\n",
            "Episode 140\tAverage Reward: -0.76\n",
            "Episode 150\tAverage Reward: -0.82\n",
            "Episode 160\tAverage Reward: -0.79\n",
            "Episode 170\tAverage Reward: -0.76\n",
            "Episode 180\tAverage Reward: -0.78\n",
            "Episode 190\tAverage Reward: -1.00\n",
            "Episode 200\tAverage Reward: -0.92\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_200.pth\n",
            "Episode 210\tAverage Reward: -0.80\n",
            "Episode 220\tAverage Reward: -0.81\n",
            "Episode 230\tAverage Reward: -0.84\n",
            "Episode 240\tAverage Reward: -0.83\n",
            "Episode 250\tAverage Reward: -0.85\n",
            "Episode 260\tAverage Reward: -0.86\n",
            "Episode 270\tAverage Reward: -0.77\n",
            "Episode 280\tAverage Reward: -0.87\n",
            "Episode 290\tAverage Reward: -0.89\n",
            "Episode 300\tAverage Reward: -0.87\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_300.pth\n",
            "Episode 310\tAverage Reward: -0.92\n",
            "Episode 320\tAverage Reward: -0.93\n",
            "Episode 330\tAverage Reward: -0.92\n",
            "Episode 340\tAverage Reward: -0.80\n",
            "Episode 350\tAverage Reward: -0.82\n",
            "Episode 360\tAverage Reward: -0.82\n",
            "Episode 370\tAverage Reward: -0.79\n",
            "Episode 380\tAverage Reward: -0.75\n",
            "Episode 390\tAverage Reward: -0.67\n",
            "Episode 400\tAverage Reward: -0.77\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_400.pth\n",
            "Episode 410\tAverage Reward: -0.75\n",
            "Episode 420\tAverage Reward: -0.80\n",
            "Episode 430\tAverage Reward: -0.82\n",
            "Episode 440\tAverage Reward: -0.84\n",
            "Episode 450\tAverage Reward: -0.91\n",
            "Episode 460\tAverage Reward: -0.89\n",
            "Episode 470\tAverage Reward: -0.97\n",
            "Episode 480\tAverage Reward: -1.00\n",
            "Episode 490\tAverage Reward: -0.91\n",
            "Episode 500\tAverage Reward: -1.01\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_500.pth\n",
            "Episode 510\tAverage Reward: -0.90\n",
            "Episode 520\tAverage Reward: -0.90\n",
            "Episode 530\tAverage Reward: -0.91\n",
            "Episode 540\tAverage Reward: -0.95\n",
            "Episode 550\tAverage Reward: -0.89\n",
            "Episode 560\tAverage Reward: -0.84\n",
            "Episode 570\tAverage Reward: -0.78\n",
            "Episode 580\tAverage Reward: -0.77\n",
            "Episode 590\tAverage Reward: -0.74\n",
            "Episode 600\tAverage Reward: -0.73\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_600.pth\n",
            "Episode 610\tAverage Reward: -0.88\n",
            "Episode 620\tAverage Reward: -0.90\n",
            "Episode 630\tAverage Reward: -0.89\n",
            "Episode 640\tAverage Reward: -0.74\n",
            "Episode 650\tAverage Reward: -0.82\n",
            "Episode 660\tAverage Reward: -0.80\n",
            "Episode 670\tAverage Reward: -0.74\n",
            "Episode 680\tAverage Reward: -0.70\n",
            "Episode 690\tAverage Reward: -0.83\n",
            "Episode 700\tAverage Reward: -0.88\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_700.pth\n",
            "Episode 710\tAverage Reward: -0.88\n",
            "Episode 720\tAverage Reward: -0.81\n",
            "Episode 730\tAverage Reward: -0.79\n",
            "Episode 740\tAverage Reward: -0.75\n",
            "Episode 750\tAverage Reward: -0.81\n",
            "Episode 760\tAverage Reward: -0.72\n",
            "Episode 770\tAverage Reward: -0.55\n",
            "Episode 780\tAverage Reward: -0.65\n",
            "Episode 790\tAverage Reward: -0.75\n",
            "Episode 800\tAverage Reward: -0.67\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_800.pth\n",
            "Episode 810\tAverage Reward: -0.75\n",
            "Episode 820\tAverage Reward: -0.77\n",
            "Episode 830\tAverage Reward: -0.86\n",
            "Episode 840\tAverage Reward: -1.00\n",
            "Episode 850\tAverage Reward: -0.90\n",
            "Episode 860\tAverage Reward: -0.79\n",
            "Episode 870\tAverage Reward: -0.76\n",
            "Episode 880\tAverage Reward: -0.75\n",
            "Episode 890\tAverage Reward: -0.76\n",
            "Episode 900\tAverage Reward: -0.78\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_900.pth\n",
            "Episode 910\tAverage Reward: -0.70\n",
            "Episode 920\tAverage Reward: -0.80\n",
            "Episode 930\tAverage Reward: -0.78\n",
            "Episode 940\tAverage Reward: -0.92\n",
            "Episode 950\tAverage Reward: -0.91\n",
            "Episode 960\tAverage Reward: -0.88\n",
            "Episode 970\tAverage Reward: -0.84\n",
            "Episode 980\tAverage Reward: -0.86\n",
            "Episode 990\tAverage Reward: -0.73\n",
            "Episode 1000\tAverage Reward: -0.82\n",
            "Modèle sauvegardé: models/sepsis_ppo_model_1000.pth\n",
            "Modèle final sauvegardé: models/sepsis_ppo_model_final.pth\n",
            "\n",
            "Évaluation du modèle final:\n",
            "Récompense moyenne sur 10 épisodes: -0.86\n",
            "\n",
            "Distribution des actions:\n",
            "  1 (Administrer fluides IV): 3 (12.0%)\n",
            "  2 (Administrer antibiotiques): 4 (16.0%)\n",
            "  3 (Administrer vasopresseurs): 11 (44.0%)\n",
            "  5 (Combo (fluides + antibiotiques)): 7 (28.0%)\n",
            "\n",
            "Entraînement et évaluation terminés!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres PPO\n",
        "GAMMA = 0.99\n",
        "LR_ACTOR = 0.0003\n",
        "LR_CRITIC = 0.001\n",
        "K_EPOCHS = 4\n",
        "EPS_CLIP = 0.2\n",
        "UPDATE_TIMESTEP = 2000\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODES = 1000\n",
        "MAX_TIMESTEPS = 100\n",
        "\n",
        "# Chemins\n",
        "DATA_PATH = \"sepsis_data.csv\"\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Environnement Sepsis\n",
        "class SepsisEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(SepsisEnv, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.n_patients = len(data)\n",
        "        self.current_patient_idx = 0\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 50  # Maximum steps per patient\n",
        "\n",
        "        # Définition des espaces d'observation et d'action\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0, 35, 0, 0, 0, 0, 0]),\n",
        "            high=np.array([200, 200, 150, 60, 41, 100, 10, 30, 24, 24]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.action_space = spaces.Discrete(6)  # 6 actions possibles\n",
        "        self.action_meanings = {\n",
        "            0: \"Ne rien faire\",\n",
        "            1: \"Administrer fluides IV\",\n",
        "            2: \"Administrer antibiotiques\",\n",
        "            3: \"Administrer vasopresseurs\",\n",
        "            4: \"Alerter clinicien\",\n",
        "            5: \"Combo (fluides + antibiotiques)\"\n",
        "        }\n",
        "\n",
        "        # Définir les limites de sepsis (simplifiée)\n",
        "        self.sepsis_thresholds = {\n",
        "            \"HR\": (90, 130),  # bpm\n",
        "            \"SysBP\": (0, 90),  # mmHg (hypotension)\n",
        "            \"Temp\": (38.3, 39.5),  # °C (fièvre)\n",
        "            \"Lactate\": (2.0, 10.0),  # mmol/L\n",
        "            \"WBC\": (12, 30)  # 10^9/L (leucocytose)\n",
        "        }\n",
        "\n",
        "        # Variables d'état internes\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Sélectionnez un patient au hasard\n",
        "        self.current_patient_idx = np.random.randint(0, self.n_patients)\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Initialiser l'état\n",
        "        self.current_state = self._get_state()\n",
        "        return self.current_state\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Obtenir les données du patient actuel à l'étape actuelle\n",
        "        if self.current_patient_idx >= len(self.data):\n",
        "            self.current_patient_idx = 0\n",
        "\n",
        "        patient_data = self.data.iloc[self.current_patient_idx]\n",
        "\n",
        "        # Créer un vecteur d'état\n",
        "        state = np.array([\n",
        "            patient_data['HR'],\n",
        "            patient_data['SysBP'],\n",
        "            patient_data['DiaBP'],\n",
        "            patient_data['Temp'],\n",
        "            patient_data['O2'],\n",
        "            patient_data['Lactate'],\n",
        "            patient_data['WBC'],\n",
        "            patient_data['Hour'],\n",
        "            patient_data['TimeSinceLastAction'],\n",
        "            self.current_step\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _calculate_reward(self, action, state_label):\n",
        "        # Base reward from state\n",
        "        if state_label == \"Stable\":\n",
        "            base_reward = 1.0\n",
        "        elif state_label == \"AtRisk\":\n",
        "            base_reward = -0.3\n",
        "        else:  # Sepsis\n",
        "            base_reward = -1.0\n",
        "\n",
        "        # Action penalty for overtreatment\n",
        "        action_penalty = 0\n",
        "        if action > 0:  # Any action except \"do nothing\"\n",
        "            # Higher penalty for stronger interventions when stable\n",
        "            if state_label == \"Stable\":\n",
        "                action_penalty = -0.2 * action  # Stronger actions get higher penalties\n",
        "            elif state_label == \"AtRisk\":\n",
        "                # Moderate penalty for strong actions, but encourage appropriate interventions\n",
        "                if action in [1, 2, 4]:  # Fluids, antibiotics, or alert are appropriate\n",
        "                    action_penalty = -0.05\n",
        "                else:\n",
        "                    action_penalty = -0.1  # Stronger penalties for vasopressors or combos\n",
        "            else:  # Sepsis\n",
        "                # In sepsis, we want strong intervention\n",
        "                if action == 5:  # Combo (fluids + antibiotics)\n",
        "                    action_penalty = 0.3  # Bonus for appropriate intensive treatment\n",
        "                elif action in [1, 2, 3]:  # Individual treatments\n",
        "                    action_penalty = 0.1  # Smaller bonus\n",
        "                else:\n",
        "                    action_penalty = -0.1  # Penalty for not treating or just alerting\n",
        "\n",
        "        return base_reward + action_penalty\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execution de l'action et calcul de la récompense\n",
        "        current_data = self.data.iloc[self.current_patient_idx]\n",
        "        state_label = current_data['StateLabel']\n",
        "        reward = self._calculate_reward(action, state_label)\n",
        "\n",
        "        # Avancer dans le temps\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Vérifier si l'épisode est terminé\n",
        "        done = False\n",
        "        if self.current_step >= self.max_steps or state_label == \"Sepsis\":\n",
        "            done = True\n",
        "\n",
        "        # Si non terminé, obtenir le nouvel état\n",
        "        if not done:\n",
        "            # Dans un environnement réel, l'état suivant dépendrait de l'action\n",
        "            # Ici, nous simulons en passant au patient suivant\n",
        "            self.current_patient_idx = (self.current_patient_idx + 1) % self.n_patients\n",
        "            next_state = self._get_state()\n",
        "        else:\n",
        "            next_state = self.current_state  # État terminal\n",
        "\n",
        "        self.current_state = next_state\n",
        "\n",
        "        info = {\n",
        "            \"state_label\": state_label,\n",
        "            \"action_taken\": self.action_meanings[action]\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Patient: {self.current_patient_idx}, Step: {self.current_step}\")\n",
        "        print(f\"State: HR={self.current_state[0]:.1f}, SysBP={self.current_state[1]:.1f}, \"\n",
        "              f\"Temp={self.current_state[3]:.1f}°C, Lactate={self.current_state[5]:.1f}\")\n",
        "\n",
        "# Définition des réseaux de neurones pour l'acteur et le critique\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # Couche partagée\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Acteur (Politique)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Critique (Fonction de valeur)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state, memory=None):\n",
        "        state = torch.from_numpy(state).float().to(DEVICE)\n",
        "        shared_features = self.shared(state)\n",
        "        action_probs = self.actor(shared_features)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        if memory is not None:\n",
        "            memory.states.append(state)\n",
        "            memory.actions.append(action)\n",
        "            memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        shared_features = self.shared(state)\n",
        "        action_probs = self.actor(shared_features)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(shared_features)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "# Mémoire pour stocker l'expérience\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "# Agent PPO\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.policy.shared.parameters(), 'lr': LR_ACTOR},\n",
        "            {'params': self.policy.actor.parameters(), 'lr': LR_ACTOR},\n",
        "            {'params': self.policy.critic.parameters(), 'lr': LR_CRITIC}\n",
        "        ])\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state, memory):\n",
        "        return self.policy_old.act(state, memory)\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimation des récompenses\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (GAMMA * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalisation des récompenses\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # Conversion des listes en tensors\n",
        "        old_states = torch.stack(memory.states).to(DEVICE).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(DEVICE).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(DEVICE).detach()\n",
        "\n",
        "        # Mise à jour des paramètres de la politique\n",
        "        for _ in range(K_EPOCHS):\n",
        "            # Évaluation des actions et des états\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Trouver le ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Fonction objectif surrogate\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-EPS_CLIP, 1+EPS_CLIP) * advantages\n",
        "\n",
        "            # Calcul de la perte\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "\n",
        "            # Rétropropagation\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copier les nouveaux poids dans le réseau de politique old\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "# Prétraitement des données\n",
        "def preprocess_data(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Normalisation des données numériques\n",
        "    numeric_columns = ['HR', 'SysBP', 'DiaBP', 'Temp', 'O2', 'Lactate', 'WBC', 'Hour', 'TimeSinceLastAction']\n",
        "    scaler = StandardScaler()\n",
        "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fonction principale d'entraînement\n",
        "def train():\n",
        "    print(\"Démarrage de l'entraînement...\")\n",
        "\n",
        "    # Charger et prétraiter les données\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "    print(f\"Données chargées: {data.shape}\")\n",
        "\n",
        "    # Créer l'environnement\n",
        "    env = SepsisEnv(data)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Créer l'agent et la mémoire\n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim)\n",
        "\n",
        "    # Variables de suivi\n",
        "    running_reward = 0\n",
        "    avg_rewards = []\n",
        "    frame_idx = 0\n",
        "    early_stop = False\n",
        "\n",
        "    # Boucle d'entraînement\n",
        "    for i_episode in range(1, MAX_EPISODES+1):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(MAX_TIMESTEPS):\n",
        "            # Sélection d'une action\n",
        "            action = ppo.select_action(state, memory)\n",
        "\n",
        "            # Exécution de l'action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Stockage des résultats\n",
        "            memory.rewards.append(reward)\n",
        "            memory.is_terminals.append(done)\n",
        "\n",
        "            frame_idx += 1\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Mise à jour si on atteint la taille de lot\n",
        "            if frame_idx % UPDATE_TIMESTEP == 0:\n",
        "                ppo.update(memory)\n",
        "                memory.clear()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Mise à jour de la récompense moyenne\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "        avg_rewards.append(running_reward)\n",
        "\n",
        "        # Affichage du progrès\n",
        "        if i_episode % 10 == 0:\n",
        "            print(f\"Episode {i_episode}\\tAverage Reward: {running_reward:.2f}\")\n",
        "\n",
        "        # Sauvegarde du modèle\n",
        "        if i_episode % 100 == 0:\n",
        "            model_path = os.path.join(MODEL_DIR, f'sepsis_ppo_model_{i_episode}.pth')\n",
        "            torch.save(ppo.policy.state_dict(), model_path)\n",
        "            print(f\"Modèle sauvegardé: {model_path}\")\n",
        "\n",
        "        # Arrêt anticipé si la performance est suffisamment bonne\n",
        "        if running_reward > 95:\n",
        "            print(f\"Problème résolu en {i_episode} épisodes!\")\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "    # Sauvegarde du modèle final\n",
        "    final_model_path = os.path.join(MODEL_DIR, 'sepsis_ppo_model_final.pth')\n",
        "    torch.save(ppo.policy.state_dict(), final_model_path)\n",
        "    print(f\"Modèle final sauvegardé: {final_model_path}\")\n",
        "\n",
        "    # Tracer la courbe d'apprentissage\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(avg_rewards)\n",
        "    plt.title('Courbe d\\'apprentissage PPO')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Récompense moyenne')\n",
        "    plt.savefig('learning_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    return ppo\n",
        "\n",
        "# Fonction d'évaluation\n",
        "def evaluate(policy, env, num_episodes=10):\n",
        "    all_rewards = []\n",
        "    all_actions = []\n",
        "    all_states = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_actions = []\n",
        "        episode_states = []\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Convertir l'état en tensor et sélectionner une action\n",
        "            state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "            shared_features = policy.shared(state_tensor)\n",
        "            action_probs = policy.actor(shared_features)\n",
        "            dist = Categorical(action_probs)\n",
        "            action = dist.sample().item()\n",
        "\n",
        "            # Exécuter l'action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Stocker les informations\n",
        "            episode_reward += reward\n",
        "            episode_actions.append(action)\n",
        "            episode_states.append(state)\n",
        "\n",
        "            # Passer à l'état suivant\n",
        "            state = next_state\n",
        "\n",
        "        all_rewards.append(episode_reward)\n",
        "        all_actions.append(episode_actions)\n",
        "        all_states.append(episode_states)\n",
        "\n",
        "    avg_reward = sum(all_rewards) / len(all_rewards)\n",
        "    print(f\"Récompense moyenne sur {num_episodes} épisodes: {avg_reward:.2f}\")\n",
        "\n",
        "    # Analyser les actions prises\n",
        "    action_counts = {}\n",
        "    for actions in all_actions:\n",
        "        for a in actions:\n",
        "            if a not in action_counts:\n",
        "                action_counts[a] = 0\n",
        "            action_counts[a] += 1\n",
        "\n",
        "    total_actions = sum(action_counts.values())\n",
        "    print(\"\\nDistribution des actions:\")\n",
        "    for a, count in sorted(action_counts.items()):\n",
        "        percentage = (count / total_actions) * 100\n",
        "        print(f\"  {a} ({env.action_meanings[a]}): {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    return avg_reward, action_counts, all_states, all_rewards\n",
        "\n",
        "# Fonction pour exécuter l'agent en production\n",
        "def run_agent(model_path, data_path):\n",
        "    # Charger les données\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # Créer l'environnement\n",
        "    env = SepsisEnv(data)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Créer et charger le modèle\n",
        "    policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "    policy.load_state_dict(torch.load(model_path))\n",
        "    policy.eval()\n",
        "\n",
        "    # Évaluer le modèle\n",
        "    avg_reward, action_counts, states, rewards = evaluate(policy, env, num_episodes=100)\n",
        "\n",
        "    return avg_reward, action_counts\n",
        "\n",
        "# Exécuter l'entraînement si lancé directement\n",
        "if __name__ == \"__main__\":\n",
        "    # Entraîner l'agent\n",
        "    trained_policy = train()\n",
        "\n",
        "    # Créer l'environnement d'évaluation\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "    eval_env = SepsisEnv(data)\n",
        "\n",
        "    # Évaluer le modèle final\n",
        "    print(\"\\nÉvaluation du modèle final:\")\n",
        "    avg_reward, action_counts, states, rewards = evaluate(trained_policy.policy, eval_env)\n",
        "\n",
        "    print(\"\\nEntraînement et évaluation terminés!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDXPmXtfXfda",
        "outputId": "405a7d0b-a5e0-4892-b0a4-49a3e509a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "         📊 Reporting Evaluation Metrics 📊\n",
            "============================================================\n",
            "\n",
            "--- Running Evaluation on 200 episodes ---\n",
            "\n",
            "--- Evaluation Results ---\n",
            "\n",
            "📈 Average Cumulative Reward per Episode: -0.83\n",
            "\n",
            "📊 Action Distribution (total over all steps):\n",
            "  0 (Ne rien faire): 44 (7.9%)\n",
            "  1 (Administrer fluides IV): 55 (9.9%)\n",
            "  2 (Administrer antibiotiques): 62 (11.2%)\n",
            "  3 (Administrer vasopresseurs): 289 (52.0%)\n",
            "  4 (Alerter clinicien): 3 (0.5%)\n",
            "  5 (Combo (fluides + antibiotiques)): 103 (18.5%)\n",
            "\n",
            "🔬 Analysis of Interventions based on State:\n",
            "Total steps evaluated: 556\n",
            "Steps in 'AtRisk' state: 174\n",
            "Steps in 'Sepsis' state: 200\n",
            "🕒 Timely interventions (appropriate actions) when 'AtRisk': 65 (37.4%)\n",
            "(Note: This is a proxy based on state label, not true clinical timeliness)\n",
            "Appropriate sepsis treatments when 'Sepsis': 181 (90.5%)\n",
            "Inaction ('Ne rien faire') when 'Sepsis': 17 (8.5%)\n",
            "\n",
            "🔍 AUC for Early Sepsis Detection:\n",
            "Note: Calculating a true AUC is not directly feasible with this environment and agent structure.\n",
            "It requires a model outputting a sepsis risk score and a dynamic environment with future outcomes.\n",
            "\n",
            "💀 Mortality Rate Reduction:\n",
            "Note: Calculating mortality rate reduction is not possible as the environment does not simulate mortality.\n",
            "\n",
            "--- Evaluation Complete ---\n",
            "\n",
            "============================================================\n",
            "       ✅ Evaluation Reporting Complete ✅\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def evaluate(policy, env, num_episodes=100): # Increased episodes for better statistics\n",
        "    # This function remains as it was in the previous turn, performing the evaluation\n",
        "    # and printing the results including reward, action distribution, and intervention analysis.\n",
        "    # We will call this function from the new reporting function.\n",
        "\n",
        "    print(f\"\\n--- Running Evaluation on {num_episodes} episodes ---\")\n",
        "    all_rewards = []\n",
        "    all_actions_taken = [] # Store the actual action index\n",
        "    all_state_labels = [] # Store the state label for each step\n",
        "\n",
        "    # Analysis counters\n",
        "    at_risk_steps = 0\n",
        "    sepsis_steps = 0\n",
        "    total_steps = 0\n",
        "    timely_interventions = 0 # Actions 1, 2, 4, 5 when AtRisk\n",
        "    sepsis_treatments = 0 # Actions 1, 2, 3, 5 when Sepsis\n",
        "    missed_sepsis_treatments = 0 # Action 0 when Sepsis\n",
        "\n",
        "    # Define which actions are considered interventions\n",
        "    # intervention_actions = [1, 2, 3, 4, 5] # All except \"Do nothing\"\n",
        "\n",
        "    # Define appropriate actions per state for analysis\n",
        "    appropriate_at_risk_actions = [1, 2, 4, 5] # Fluids, Antibiotics, Alert, Combo\n",
        "    appropriate_sepsis_actions = [1, 2, 3, 5] # Fluids, Antibiotics, Vasopressors, Combo\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_actions = []\n",
        "        episode_state_labels = []\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Select an action using the policy (in evaluation mode)\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE) # Add batch dim\n",
        "            # Need to get action_probs to sample, or just take the argmax for deterministic eval\n",
        "            # For evaluation metrics like action distribution, sampling is better.\n",
        "            # For comparing performance, deterministic (argmax) might be used.\n",
        "            # Let's sample for action distribution analysis.\n",
        "            with torch.no_grad(): # No gradients needed during evaluation\n",
        "                 shared_features = policy.shared(state_tensor)\n",
        "                 action_probs = policy.actor(shared_features)\n",
        "                 dist = Categorical(action_probs)\n",
        "                 action = dist.sample().item() # Sample action\n",
        "\n",
        "            # Exécuter l'action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store information for analysis\n",
        "            episode_reward += reward\n",
        "            episode_actions.append(action)\n",
        "            episode_state_labels.append(info['state_label']) # Store the label of the state *before* the action was taken\n",
        "\n",
        "            # Count state occurrences and interventions\n",
        "            total_steps += 1\n",
        "            if info['state_label'] == \"AtRisk\":\n",
        "                at_risk_steps += 1\n",
        "                if action in appropriate_at_risk_actions:\n",
        "                    timely_interventions += 1\n",
        "            elif info['state_label'] == \"Sepsis\":\n",
        "                sepsis_steps += 1\n",
        "                if action in appropriate_sepsis_actions:\n",
        "                    sepsis_treatments += 1\n",
        "                if action == 0:\n",
        "                     missed_sepsis_treatments += 1\n",
        "\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "        all_rewards.append(episode_reward)\n",
        "        all_actions_taken.extend(episode_actions) # Flatten the list of actions\n",
        "        all_state_labels.extend(episode_state_labels) # Flatten the list of state labels\n",
        "\n",
        "    # --- Print the metrics ---\n",
        "    print(f\"\\n--- Evaluation Results ---\")\n",
        "\n",
        "    # 📈 Cumulative reward per episode\n",
        "    avg_reward = sum(all_rewards) / len(all_rewards) if all_rewards else 0\n",
        "    print(f\"\\n📈 Average Cumulative Reward per Episode: {avg_reward:.2f}\")\n",
        "\n",
        "    # Action Distribution\n",
        "    action_counts = {}\n",
        "    for a in all_actions_taken:\n",
        "        action_counts[a] = action_counts.get(a, 0) + 1\n",
        "\n",
        "    print(\"\\n📊 Action Distribution (total over all steps):\")\n",
        "    total_actions = len(all_actions_taken)\n",
        "    for a in sorted(env.action_meanings.keys()):\n",
        "         count = action_counts.get(a, 0)\n",
        "         percentage = (count / total_actions) * 100 if total_actions > 0 else 0\n",
        "         print(f\"  {a} ({env.action_meanings[a]}): {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # 🕒 Number of timely interventions made (Proxy) & Sepsis Treatment\n",
        "    print(\"\\n🔬 Analysis of Interventions based on State:\")\n",
        "\n",
        "    print(f\"Total steps evaluated: {total_steps}\")\n",
        "    print(f\"Steps in 'AtRisk' state: {at_risk_steps}\")\n",
        "    print(f\"Steps in 'Sepsis' state: {sepsis_steps}\")\n",
        "\n",
        "    # Timely interventions when AtRisk\n",
        "    timely_intervention_rate = (timely_interventions / at_risk_steps) * 100 if at_risk_steps > 0 else 0\n",
        "    print(f\"🕒 Timely interventions (appropriate actions) when 'AtRisk': {timely_interventions} ({timely_intervention_rate:.1f}%)\")\n",
        "    print(\"(Note: This is a proxy based on state label, not true clinical timeliness)\")\n",
        "\n",
        "    # Sepsis treatments when Sepsis\n",
        "    sepsis_treatment_rate = (sepsis_treatments / sepsis_steps) * 100 if sepsis_steps > 0 else 0\n",
        "    print(f\"Appropriate sepsis treatments when 'Sepsis': {sepsis_treatments} ({sepsis_treatment_rate:.1f}%)\")\n",
        "\n",
        "    # Missed sepsis treatments (inaction) when Sepsis\n",
        "    missed_sepsis_treatment_rate = (missed_sepsis_treatments / sepsis_steps) * 100 if sepsis_steps > 0 else 0\n",
        "    print(f\"Inaction ('Ne rien faire') when 'Sepsis': {missed_sepsis_treatments} ({missed_sepsis_treatment_rate:.1f}%)\")\n",
        "\n",
        "    # 🔍 AUC for early sepsis detection (Discussion)\n",
        "    print(\"\\n🔍 AUC for Early Sepsis Detection:\")\n",
        "    print(\"Note: Calculating a true AUC is not directly feasible with this environment and agent structure.\")\n",
        "    print(\"It requires a model outputting a sepsis risk score and a dynamic environment with future outcomes.\")\n",
        "\n",
        "    # 💀 Mortality rate reduction (Discussion)\n",
        "    print(\"\\n💀 Mortality Rate Reduction:\")\n",
        "    print(\"Note: Calculating mortality rate reduction is not possible as the environment does not simulate mortality.\")\n",
        "\n",
        "    print(f\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "    # You can return these metrics if you want to process them further outside this function\n",
        "    return avg_reward, action_counts, timely_intervention_rate, sepsis_treatment_rate, missed_sepsis_treatment_rate\n",
        "\n",
        "\n",
        "def report_evaluation_metrics(policy, env, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Dedicated function to evaluate the trained policy and report the specified metrics.\n",
        "\n",
        "    Args:\n",
        "        policy: The trained ActorCritic policy model.\n",
        "        env: The SepsisEnv environment configured for evaluation.\n",
        "        num_episodes: Number of episodes to run for evaluation statistics.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"         📊 Reporting Evaluation Metrics 📊\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # The 'evaluate' function already calculates and prints the required metrics\n",
        "    # including Average Cumulative Reward, Action Distribution, Timely Interventions\n",
        "    # (proxy), Sepsis Treatment analysis, and notes on AUC/Mortality limitations.\n",
        "    # We just need to call it here.\n",
        "\n",
        "    evaluate(policy.policy, env, num_episodes=num_episodes)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"       ✅ Evaluation Reporting Complete ✅\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "report_evaluation_metrics(trained_policy, eval_env, num_episodes=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Blj8NlHm6K6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
