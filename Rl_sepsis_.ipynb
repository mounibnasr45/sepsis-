{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wR3tUWISJlP8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWXuSvjCKbGO",
        "outputId": "ec717a81-d8c8-494f-b9dd-372273415cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "D√©marrage de l'entra√Ænement...\n",
            "Donn√©es charg√©es: (500, 11)\n",
            "Episode 10\tAverage Reward: -0.39\n",
            "Episode 20\tAverage Reward: -0.53\n",
            "Episode 30\tAverage Reward: -0.63\n",
            "Episode 40\tAverage Reward: -0.73\n",
            "Episode 50\tAverage Reward: -0.77\n",
            "Episode 60\tAverage Reward: -0.86\n",
            "Episode 70\tAverage Reward: -0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 80\tAverage Reward: -0.73\n",
            "Episode 90\tAverage Reward: -0.73\n",
            "Episode 100\tAverage Reward: -0.75\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_100.pth\n",
            "Episode 110\tAverage Reward: -0.73\n",
            "Episode 120\tAverage Reward: -0.70\n",
            "Episode 130\tAverage Reward: -0.82\n",
            "Episode 140\tAverage Reward: -0.76\n",
            "Episode 150\tAverage Reward: -0.82\n",
            "Episode 160\tAverage Reward: -0.79\n",
            "Episode 170\tAverage Reward: -0.76\n",
            "Episode 180\tAverage Reward: -0.78\n",
            "Episode 190\tAverage Reward: -1.00\n",
            "Episode 200\tAverage Reward: -0.92\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_200.pth\n",
            "Episode 210\tAverage Reward: -0.80\n",
            "Episode 220\tAverage Reward: -0.81\n",
            "Episode 230\tAverage Reward: -0.84\n",
            "Episode 240\tAverage Reward: -0.83\n",
            "Episode 250\tAverage Reward: -0.85\n",
            "Episode 260\tAverage Reward: -0.86\n",
            "Episode 270\tAverage Reward: -0.77\n",
            "Episode 280\tAverage Reward: -0.87\n",
            "Episode 290\tAverage Reward: -0.89\n",
            "Episode 300\tAverage Reward: -0.87\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_300.pth\n",
            "Episode 310\tAverage Reward: -0.92\n",
            "Episode 320\tAverage Reward: -0.93\n",
            "Episode 330\tAverage Reward: -0.92\n",
            "Episode 340\tAverage Reward: -0.80\n",
            "Episode 350\tAverage Reward: -0.82\n",
            "Episode 360\tAverage Reward: -0.82\n",
            "Episode 370\tAverage Reward: -0.79\n",
            "Episode 380\tAverage Reward: -0.75\n",
            "Episode 390\tAverage Reward: -0.67\n",
            "Episode 400\tAverage Reward: -0.77\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_400.pth\n",
            "Episode 410\tAverage Reward: -0.75\n",
            "Episode 420\tAverage Reward: -0.80\n",
            "Episode 430\tAverage Reward: -0.82\n",
            "Episode 440\tAverage Reward: -0.84\n",
            "Episode 450\tAverage Reward: -0.91\n",
            "Episode 460\tAverage Reward: -0.89\n",
            "Episode 470\tAverage Reward: -0.97\n",
            "Episode 480\tAverage Reward: -1.00\n",
            "Episode 490\tAverage Reward: -0.91\n",
            "Episode 500\tAverage Reward: -1.01\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_500.pth\n",
            "Episode 510\tAverage Reward: -0.90\n",
            "Episode 520\tAverage Reward: -0.90\n",
            "Episode 530\tAverage Reward: -0.91\n",
            "Episode 540\tAverage Reward: -0.95\n",
            "Episode 550\tAverage Reward: -0.89\n",
            "Episode 560\tAverage Reward: -0.84\n",
            "Episode 570\tAverage Reward: -0.78\n",
            "Episode 580\tAverage Reward: -0.77\n",
            "Episode 590\tAverage Reward: -0.74\n",
            "Episode 600\tAverage Reward: -0.73\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_600.pth\n",
            "Episode 610\tAverage Reward: -0.88\n",
            "Episode 620\tAverage Reward: -0.90\n",
            "Episode 630\tAverage Reward: -0.89\n",
            "Episode 640\tAverage Reward: -0.74\n",
            "Episode 650\tAverage Reward: -0.82\n",
            "Episode 660\tAverage Reward: -0.80\n",
            "Episode 670\tAverage Reward: -0.74\n",
            "Episode 680\tAverage Reward: -0.70\n",
            "Episode 690\tAverage Reward: -0.83\n",
            "Episode 700\tAverage Reward: -0.88\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_700.pth\n",
            "Episode 710\tAverage Reward: -0.88\n",
            "Episode 720\tAverage Reward: -0.81\n",
            "Episode 730\tAverage Reward: -0.79\n",
            "Episode 740\tAverage Reward: -0.75\n",
            "Episode 750\tAverage Reward: -0.81\n",
            "Episode 760\tAverage Reward: -0.72\n",
            "Episode 770\tAverage Reward: -0.55\n",
            "Episode 780\tAverage Reward: -0.65\n",
            "Episode 790\tAverage Reward: -0.75\n",
            "Episode 800\tAverage Reward: -0.67\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_800.pth\n",
            "Episode 810\tAverage Reward: -0.75\n",
            "Episode 820\tAverage Reward: -0.77\n",
            "Episode 830\tAverage Reward: -0.86\n",
            "Episode 840\tAverage Reward: -1.00\n",
            "Episode 850\tAverage Reward: -0.90\n",
            "Episode 860\tAverage Reward: -0.79\n",
            "Episode 870\tAverage Reward: -0.76\n",
            "Episode 880\tAverage Reward: -0.75\n",
            "Episode 890\tAverage Reward: -0.76\n",
            "Episode 900\tAverage Reward: -0.78\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_900.pth\n",
            "Episode 910\tAverage Reward: -0.70\n",
            "Episode 920\tAverage Reward: -0.80\n",
            "Episode 930\tAverage Reward: -0.78\n",
            "Episode 940\tAverage Reward: -0.92\n",
            "Episode 950\tAverage Reward: -0.91\n",
            "Episode 960\tAverage Reward: -0.88\n",
            "Episode 970\tAverage Reward: -0.84\n",
            "Episode 980\tAverage Reward: -0.86\n",
            "Episode 990\tAverage Reward: -0.73\n",
            "Episode 1000\tAverage Reward: -0.82\n",
            "Mod√®le sauvegard√©: models/sepsis_ppo_model_1000.pth\n",
            "Mod√®le final sauvegard√©: models/sepsis_ppo_model_final.pth\n",
            "\n",
            "√âvaluation du mod√®le final:\n",
            "R√©compense moyenne sur 10 √©pisodes: -0.86\n",
            "\n",
            "Distribution des actions:\n",
            "  1 (Administrer fluides IV): 3 (12.0%)\n",
            "  2 (Administrer antibiotiques): 4 (16.0%)\n",
            "  3 (Administrer vasopresseurs): 11 (44.0%)\n",
            "  5 (Combo (fluides + antibiotiques)): 7 (28.0%)\n",
            "\n",
            "Entra√Ænement et √©valuation termin√©s!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparam√®tres PPO\n",
        "GAMMA = 0.99\n",
        "LR_ACTOR = 0.0003\n",
        "LR_CRITIC = 0.001\n",
        "K_EPOCHS = 4\n",
        "EPS_CLIP = 0.2\n",
        "UPDATE_TIMESTEP = 2000\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODES = 1000\n",
        "MAX_TIMESTEPS = 100\n",
        "\n",
        "# Chemins\n",
        "DATA_PATH = \"sepsis_data.csv\"\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Environnement Sepsis\n",
        "class SepsisEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(SepsisEnv, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.n_patients = len(data)\n",
        "        self.current_patient_idx = 0\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 50  # Maximum steps per patient\n",
        "\n",
        "        # D√©finition des espaces d'observation et d'action\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0, 35, 0, 0, 0, 0, 0]),\n",
        "            high=np.array([200, 200, 150, 60, 41, 100, 10, 30, 24, 24]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.action_space = spaces.Discrete(6)  # 6 actions possibles\n",
        "        self.action_meanings = {\n",
        "            0: \"Ne rien faire\",\n",
        "            1: \"Administrer fluides IV\",\n",
        "            2: \"Administrer antibiotiques\",\n",
        "            3: \"Administrer vasopresseurs\",\n",
        "            4: \"Alerter clinicien\",\n",
        "            5: \"Combo (fluides + antibiotiques)\"\n",
        "        }\n",
        "\n",
        "        # D√©finir les limites de sepsis (simplifi√©e)\n",
        "        self.sepsis_thresholds = {\n",
        "            \"HR\": (90, 130),  # bpm\n",
        "            \"SysBP\": (0, 90),  # mmHg (hypotension)\n",
        "            \"Temp\": (38.3, 39.5),  # ¬∞C (fi√®vre)\n",
        "            \"Lactate\": (2.0, 10.0),  # mmol/L\n",
        "            \"WBC\": (12, 30)  # 10^9/L (leucocytose)\n",
        "        }\n",
        "\n",
        "        # Variables d'√©tat internes\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # S√©lectionnez un patient au hasard\n",
        "        self.current_patient_idx = np.random.randint(0, self.n_patients)\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Initialiser l'√©tat\n",
        "        self.current_state = self._get_state()\n",
        "        return self.current_state\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Obtenir les donn√©es du patient actuel √† l'√©tape actuelle\n",
        "        if self.current_patient_idx >= len(self.data):\n",
        "            self.current_patient_idx = 0\n",
        "\n",
        "        patient_data = self.data.iloc[self.current_patient_idx]\n",
        "\n",
        "        # Cr√©er un vecteur d'√©tat\n",
        "        state = np.array([\n",
        "            patient_data['HR'],\n",
        "            patient_data['SysBP'],\n",
        "            patient_data['DiaBP'],\n",
        "            patient_data['Temp'],\n",
        "            patient_data['O2'],\n",
        "            patient_data['Lactate'],\n",
        "            patient_data['WBC'],\n",
        "            patient_data['Hour'],\n",
        "            patient_data['TimeSinceLastAction'],\n",
        "            self.current_step\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _calculate_reward(self, action, state_label):\n",
        "        # Base reward from state\n",
        "        if state_label == \"Stable\":\n",
        "            base_reward = 1.0\n",
        "        elif state_label == \"AtRisk\":\n",
        "            base_reward = -0.3\n",
        "        else:  # Sepsis\n",
        "            base_reward = -1.0\n",
        "\n",
        "        # Action penalty for overtreatment\n",
        "        action_penalty = 0\n",
        "        if action > 0:  # Any action except \"do nothing\"\n",
        "            # Higher penalty for stronger interventions when stable\n",
        "            if state_label == \"Stable\":\n",
        "                action_penalty = -0.2 * action  # Stronger actions get higher penalties\n",
        "            elif state_label == \"AtRisk\":\n",
        "                # Moderate penalty for strong actions, but encourage appropriate interventions\n",
        "                if action in [1, 2, 4]:  # Fluids, antibiotics, or alert are appropriate\n",
        "                    action_penalty = -0.05\n",
        "                else:\n",
        "                    action_penalty = -0.1  # Stronger penalties for vasopressors or combos\n",
        "            else:  # Sepsis\n",
        "                # In sepsis, we want strong intervention\n",
        "                if action == 5:  # Combo (fluids + antibiotics)\n",
        "                    action_penalty = 0.3  # Bonus for appropriate intensive treatment\n",
        "                elif action in [1, 2, 3]:  # Individual treatments\n",
        "                    action_penalty = 0.1  # Smaller bonus\n",
        "                else:\n",
        "                    action_penalty = -0.1  # Penalty for not treating or just alerting\n",
        "\n",
        "        return base_reward + action_penalty\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execution de l'action et calcul de la r√©compense\n",
        "        current_data = self.data.iloc[self.current_patient_idx]\n",
        "        state_label = current_data['StateLabel']\n",
        "        reward = self._calculate_reward(action, state_label)\n",
        "\n",
        "        # Avancer dans le temps\n",
        "        self.current_step += 1\n",
        "\n",
        "        # V√©rifier si l'√©pisode est termin√©\n",
        "        done = False\n",
        "        if self.current_step >= self.max_steps or state_label == \"Sepsis\":\n",
        "            done = True\n",
        "\n",
        "        # Si non termin√©, obtenir le nouvel √©tat\n",
        "        if not done:\n",
        "            # Dans un environnement r√©el, l'√©tat suivant d√©pendrait de l'action\n",
        "            # Ici, nous simulons en passant au patient suivant\n",
        "            self.current_patient_idx = (self.current_patient_idx + 1) % self.n_patients\n",
        "            next_state = self._get_state()\n",
        "        else:\n",
        "            next_state = self.current_state  # √âtat terminal\n",
        "\n",
        "        self.current_state = next_state\n",
        "\n",
        "        info = {\n",
        "            \"state_label\": state_label,\n",
        "            \"action_taken\": self.action_meanings[action]\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Patient: {self.current_patient_idx}, Step: {self.current_step}\")\n",
        "        print(f\"State: HR={self.current_state[0]:.1f}, SysBP={self.current_state[1]:.1f}, \"\n",
        "              f\"Temp={self.current_state[3]:.1f}¬∞C, Lactate={self.current_state[5]:.1f}\")\n",
        "\n",
        "# D√©finition des r√©seaux de neurones pour l'acteur et le critique\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # Couche partag√©e\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Acteur (Politique)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Critique (Fonction de valeur)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state, memory=None):\n",
        "        state = torch.from_numpy(state).float().to(DEVICE)\n",
        "        shared_features = self.shared(state)\n",
        "        action_probs = self.actor(shared_features)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        if memory is not None:\n",
        "            memory.states.append(state)\n",
        "            memory.actions.append(action)\n",
        "            memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        shared_features = self.shared(state)\n",
        "        action_probs = self.actor(shared_features)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(shared_features)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "# M√©moire pour stocker l'exp√©rience\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "# Agent PPO\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.policy.shared.parameters(), 'lr': LR_ACTOR},\n",
        "            {'params': self.policy.actor.parameters(), 'lr': LR_ACTOR},\n",
        "            {'params': self.policy.critic.parameters(), 'lr': LR_CRITIC}\n",
        "        ])\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state, memory):\n",
        "        return self.policy_old.act(state, memory)\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimation des r√©compenses\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (GAMMA * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalisation des r√©compenses\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # Conversion des listes en tensors\n",
        "        old_states = torch.stack(memory.states).to(DEVICE).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(DEVICE).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(DEVICE).detach()\n",
        "\n",
        "        # Mise √† jour des param√®tres de la politique\n",
        "        for _ in range(K_EPOCHS):\n",
        "            # √âvaluation des actions et des √©tats\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Trouver le ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Fonction objectif surrogate\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-EPS_CLIP, 1+EPS_CLIP) * advantages\n",
        "\n",
        "            # Calcul de la perte\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "\n",
        "            # R√©tropropagation\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copier les nouveaux poids dans le r√©seau de politique old\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "# Pr√©traitement des donn√©es\n",
        "def preprocess_data(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Normalisation des donn√©es num√©riques\n",
        "    numeric_columns = ['HR', 'SysBP', 'DiaBP', 'Temp', 'O2', 'Lactate', 'WBC', 'Hour', 'TimeSinceLastAction']\n",
        "    scaler = StandardScaler()\n",
        "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fonction principale d'entra√Ænement\n",
        "def train():\n",
        "    print(\"D√©marrage de l'entra√Ænement...\")\n",
        "\n",
        "    # Charger et pr√©traiter les donn√©es\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "    print(f\"Donn√©es charg√©es: {data.shape}\")\n",
        "\n",
        "    # Cr√©er l'environnement\n",
        "    env = SepsisEnv(data)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Cr√©er l'agent et la m√©moire\n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim)\n",
        "\n",
        "    # Variables de suivi\n",
        "    running_reward = 0\n",
        "    avg_rewards = []\n",
        "    frame_idx = 0\n",
        "    early_stop = False\n",
        "\n",
        "    # Boucle d'entra√Ænement\n",
        "    for i_episode in range(1, MAX_EPISODES+1):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(MAX_TIMESTEPS):\n",
        "            # S√©lection d'une action\n",
        "            action = ppo.select_action(state, memory)\n",
        "\n",
        "            # Ex√©cution de l'action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Stockage des r√©sultats\n",
        "            memory.rewards.append(reward)\n",
        "            memory.is_terminals.append(done)\n",
        "\n",
        "            frame_idx += 1\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Mise √† jour si on atteint la taille de lot\n",
        "            if frame_idx % UPDATE_TIMESTEP == 0:\n",
        "                ppo.update(memory)\n",
        "                memory.clear()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Mise √† jour de la r√©compense moyenne\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "        avg_rewards.append(running_reward)\n",
        "\n",
        "        # Affichage du progr√®s\n",
        "        if i_episode % 10 == 0:\n",
        "            print(f\"Episode {i_episode}\\tAverage Reward: {running_reward:.2f}\")\n",
        "\n",
        "        # Sauvegarde du mod√®le\n",
        "        if i_episode % 100 == 0:\n",
        "            model_path = os.path.join(MODEL_DIR, f'sepsis_ppo_model_{i_episode}.pth')\n",
        "            torch.save(ppo.policy.state_dict(), model_path)\n",
        "            print(f\"Mod√®le sauvegard√©: {model_path}\")\n",
        "\n",
        "        # Arr√™t anticip√© si la performance est suffisamment bonne\n",
        "        if running_reward > 95:\n",
        "            print(f\"Probl√®me r√©solu en {i_episode} √©pisodes!\")\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "    # Sauvegarde du mod√®le final\n",
        "    final_model_path = os.path.join(MODEL_DIR, 'sepsis_ppo_model_final.pth')\n",
        "    torch.save(ppo.policy.state_dict(), final_model_path)\n",
        "    print(f\"Mod√®le final sauvegard√©: {final_model_path}\")\n",
        "\n",
        "    # Tracer la courbe d'apprentissage\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(avg_rewards)\n",
        "    plt.title('Courbe d\\'apprentissage PPO')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('R√©compense moyenne')\n",
        "    plt.savefig('learning_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    return ppo\n",
        "\n",
        "# Fonction d'√©valuation\n",
        "def evaluate(policy, env, num_episodes=10):\n",
        "    all_rewards = []\n",
        "    all_actions = []\n",
        "    all_states = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_actions = []\n",
        "        episode_states = []\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Convertir l'√©tat en tensor et s√©lectionner une action\n",
        "            state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "            shared_features = policy.shared(state_tensor)\n",
        "            action_probs = policy.actor(shared_features)\n",
        "            dist = Categorical(action_probs)\n",
        "            action = dist.sample().item()\n",
        "\n",
        "            # Ex√©cuter l'action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Stocker les informations\n",
        "            episode_reward += reward\n",
        "            episode_actions.append(action)\n",
        "            episode_states.append(state)\n",
        "\n",
        "            # Passer √† l'√©tat suivant\n",
        "            state = next_state\n",
        "\n",
        "        all_rewards.append(episode_reward)\n",
        "        all_actions.append(episode_actions)\n",
        "        all_states.append(episode_states)\n",
        "\n",
        "    avg_reward = sum(all_rewards) / len(all_rewards)\n",
        "    print(f\"R√©compense moyenne sur {num_episodes} √©pisodes: {avg_reward:.2f}\")\n",
        "\n",
        "    # Analyser les actions prises\n",
        "    action_counts = {}\n",
        "    for actions in all_actions:\n",
        "        for a in actions:\n",
        "            if a not in action_counts:\n",
        "                action_counts[a] = 0\n",
        "            action_counts[a] += 1\n",
        "\n",
        "    total_actions = sum(action_counts.values())\n",
        "    print(\"\\nDistribution des actions:\")\n",
        "    for a, count in sorted(action_counts.items()):\n",
        "        percentage = (count / total_actions) * 100\n",
        "        print(f\"  {a} ({env.action_meanings[a]}): {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    return avg_reward, action_counts, all_states, all_rewards\n",
        "\n",
        "# Fonction pour ex√©cuter l'agent en production\n",
        "def run_agent(model_path, data_path):\n",
        "    # Charger les donn√©es\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # Cr√©er l'environnement\n",
        "    env = SepsisEnv(data)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Cr√©er et charger le mod√®le\n",
        "    policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "    policy.load_state_dict(torch.load(model_path))\n",
        "    policy.eval()\n",
        "\n",
        "    # √âvaluer le mod√®le\n",
        "    avg_reward, action_counts, states, rewards = evaluate(policy, env, num_episodes=100)\n",
        "\n",
        "    return avg_reward, action_counts\n",
        "\n",
        "# Ex√©cuter l'entra√Ænement si lanc√© directement\n",
        "if __name__ == \"__main__\":\n",
        "    # Entra√Æner l'agent\n",
        "    trained_policy = train()\n",
        "\n",
        "    # Cr√©er l'environnement d'√©valuation\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "    eval_env = SepsisEnv(data)\n",
        "\n",
        "    # √âvaluer le mod√®le final\n",
        "    print(\"\\n√âvaluation du mod√®le final:\")\n",
        "    avg_reward, action_counts, states, rewards = evaluate(trained_policy.policy, eval_env)\n",
        "\n",
        "    print(\"\\nEntra√Ænement et √©valuation termin√©s!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDXPmXtfXfda",
        "outputId": "405a7d0b-a5e0-4892-b0a4-49a3e509a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "         üìä Reporting Evaluation Metrics üìä\n",
            "============================================================\n",
            "\n",
            "--- Running Evaluation on 200 episodes ---\n",
            "\n",
            "--- Evaluation Results ---\n",
            "\n",
            "üìà Average Cumulative Reward per Episode: -0.83\n",
            "\n",
            "üìä Action Distribution (total over all steps):\n",
            "  0 (Ne rien faire): 44 (7.9%)\n",
            "  1 (Administrer fluides IV): 55 (9.9%)\n",
            "  2 (Administrer antibiotiques): 62 (11.2%)\n",
            "  3 (Administrer vasopresseurs): 289 (52.0%)\n",
            "  4 (Alerter clinicien): 3 (0.5%)\n",
            "  5 (Combo (fluides + antibiotiques)): 103 (18.5%)\n",
            "\n",
            "üî¨ Analysis of Interventions based on State:\n",
            "Total steps evaluated: 556\n",
            "Steps in 'AtRisk' state: 174\n",
            "Steps in 'Sepsis' state: 200\n",
            "üïí Timely interventions (appropriate actions) when 'AtRisk': 65 (37.4%)\n",
            "(Note: This is a proxy based on state label, not true clinical timeliness)\n",
            "Appropriate sepsis treatments when 'Sepsis': 181 (90.5%)\n",
            "Inaction ('Ne rien faire') when 'Sepsis': 17 (8.5%)\n",
            "\n",
            "üîç AUC for Early Sepsis Detection:\n",
            "Note: Calculating a true AUC is not directly feasible with this environment and agent structure.\n",
            "It requires a model outputting a sepsis risk score and a dynamic environment with future outcomes.\n",
            "\n",
            "üíÄ Mortality Rate Reduction:\n",
            "Note: Calculating mortality rate reduction is not possible as the environment does not simulate mortality.\n",
            "\n",
            "--- Evaluation Complete ---\n",
            "\n",
            "============================================================\n",
            "       ‚úÖ Evaluation Reporting Complete ‚úÖ\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def evaluate(policy, env, num_episodes=100): # Increased episodes for better statistics\n",
        "    # This function remains as it was in the previous turn, performing the evaluation\n",
        "    # and printing the results including reward, action distribution, and intervention analysis.\n",
        "    # We will call this function from the new reporting function.\n",
        "\n",
        "    print(f\"\\n--- Running Evaluation on {num_episodes} episodes ---\")\n",
        "    all_rewards = []\n",
        "    all_actions_taken = [] # Store the actual action index\n",
        "    all_state_labels = [] # Store the state label for each step\n",
        "\n",
        "    # Analysis counters\n",
        "    at_risk_steps = 0\n",
        "    sepsis_steps = 0\n",
        "    total_steps = 0\n",
        "    timely_interventions = 0 # Actions 1, 2, 4, 5 when AtRisk\n",
        "    sepsis_treatments = 0 # Actions 1, 2, 3, 5 when Sepsis\n",
        "    missed_sepsis_treatments = 0 # Action 0 when Sepsis\n",
        "\n",
        "    # Define which actions are considered interventions\n",
        "    # intervention_actions = [1, 2, 3, 4, 5] # All except \"Do nothing\"\n",
        "\n",
        "    # Define appropriate actions per state for analysis\n",
        "    appropriate_at_risk_actions = [1, 2, 4, 5] # Fluids, Antibiotics, Alert, Combo\n",
        "    appropriate_sepsis_actions = [1, 2, 3, 5] # Fluids, Antibiotics, Vasopressors, Combo\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_actions = []\n",
        "        episode_state_labels = []\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Select an action using the policy (in evaluation mode)\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE) # Add batch dim\n",
        "            # Need to get action_probs to sample, or just take the argmax for deterministic eval\n",
        "            # For evaluation metrics like action distribution, sampling is better.\n",
        "            # For comparing performance, deterministic (argmax) might be used.\n",
        "            # Let's sample for action distribution analysis.\n",
        "            with torch.no_grad(): # No gradients needed during evaluation\n",
        "                 shared_features = policy.shared(state_tensor)\n",
        "                 action_probs = policy.actor(shared_features)\n",
        "                 dist = Categorical(action_probs)\n",
        "                 action = dist.sample().item() # Sample action\n",
        "\n",
        "            # Ex√©cuter l'action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store information for analysis\n",
        "            episode_reward += reward\n",
        "            episode_actions.append(action)\n",
        "            episode_state_labels.append(info['state_label']) # Store the label of the state *before* the action was taken\n",
        "\n",
        "            # Count state occurrences and interventions\n",
        "            total_steps += 1\n",
        "            if info['state_label'] == \"AtRisk\":\n",
        "                at_risk_steps += 1\n",
        "                if action in appropriate_at_risk_actions:\n",
        "                    timely_interventions += 1\n",
        "            elif info['state_label'] == \"Sepsis\":\n",
        "                sepsis_steps += 1\n",
        "                if action in appropriate_sepsis_actions:\n",
        "                    sepsis_treatments += 1\n",
        "                if action == 0:\n",
        "                     missed_sepsis_treatments += 1\n",
        "\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "        all_rewards.append(episode_reward)\n",
        "        all_actions_taken.extend(episode_actions) # Flatten the list of actions\n",
        "        all_state_labels.extend(episode_state_labels) # Flatten the list of state labels\n",
        "\n",
        "    # --- Print the metrics ---\n",
        "    print(f\"\\n--- Evaluation Results ---\")\n",
        "\n",
        "    # üìà Cumulative reward per episode\n",
        "    avg_reward = sum(all_rewards) / len(all_rewards) if all_rewards else 0\n",
        "    print(f\"\\nüìà Average Cumulative Reward per Episode: {avg_reward:.2f}\")\n",
        "\n",
        "    # Action Distribution\n",
        "    action_counts = {}\n",
        "    for a in all_actions_taken:\n",
        "        action_counts[a] = action_counts.get(a, 0) + 1\n",
        "\n",
        "    print(\"\\nüìä Action Distribution (total over all steps):\")\n",
        "    total_actions = len(all_actions_taken)\n",
        "    for a in sorted(env.action_meanings.keys()):\n",
        "         count = action_counts.get(a, 0)\n",
        "         percentage = (count / total_actions) * 100 if total_actions > 0 else 0\n",
        "         print(f\"  {a} ({env.action_meanings[a]}): {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # üïí Number of timely interventions made (Proxy) & Sepsis Treatment\n",
        "    print(\"\\nüî¨ Analysis of Interventions based on State:\")\n",
        "\n",
        "    print(f\"Total steps evaluated: {total_steps}\")\n",
        "    print(f\"Steps in 'AtRisk' state: {at_risk_steps}\")\n",
        "    print(f\"Steps in 'Sepsis' state: {sepsis_steps}\")\n",
        "\n",
        "    # Timely interventions when AtRisk\n",
        "    timely_intervention_rate = (timely_interventions / at_risk_steps) * 100 if at_risk_steps > 0 else 0\n",
        "    print(f\"üïí Timely interventions (appropriate actions) when 'AtRisk': {timely_interventions} ({timely_intervention_rate:.1f}%)\")\n",
        "    print(\"(Note: This is a proxy based on state label, not true clinical timeliness)\")\n",
        "\n",
        "    # Sepsis treatments when Sepsis\n",
        "    sepsis_treatment_rate = (sepsis_treatments / sepsis_steps) * 100 if sepsis_steps > 0 else 0\n",
        "    print(f\"Appropriate sepsis treatments when 'Sepsis': {sepsis_treatments} ({sepsis_treatment_rate:.1f}%)\")\n",
        "\n",
        "    # Missed sepsis treatments (inaction) when Sepsis\n",
        "    missed_sepsis_treatment_rate = (missed_sepsis_treatments / sepsis_steps) * 100 if sepsis_steps > 0 else 0\n",
        "    print(f\"Inaction ('Ne rien faire') when 'Sepsis': {missed_sepsis_treatments} ({missed_sepsis_treatment_rate:.1f}%)\")\n",
        "\n",
        "    # üîç AUC for early sepsis detection (Discussion)\n",
        "    print(\"\\nüîç AUC for Early Sepsis Detection:\")\n",
        "    print(\"Note: Calculating a true AUC is not directly feasible with this environment and agent structure.\")\n",
        "    print(\"It requires a model outputting a sepsis risk score and a dynamic environment with future outcomes.\")\n",
        "\n",
        "    # üíÄ Mortality rate reduction (Discussion)\n",
        "    print(\"\\nüíÄ Mortality Rate Reduction:\")\n",
        "    print(\"Note: Calculating mortality rate reduction is not possible as the environment does not simulate mortality.\")\n",
        "\n",
        "    print(f\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "    # You can return these metrics if you want to process them further outside this function\n",
        "    return avg_reward, action_counts, timely_intervention_rate, sepsis_treatment_rate, missed_sepsis_treatment_rate\n",
        "\n",
        "\n",
        "def report_evaluation_metrics(policy, env, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Dedicated function to evaluate the trained policy and report the specified metrics.\n",
        "\n",
        "    Args:\n",
        "        policy: The trained ActorCritic policy model.\n",
        "        env: The SepsisEnv environment configured for evaluation.\n",
        "        num_episodes: Number of episodes to run for evaluation statistics.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"         üìä Reporting Evaluation Metrics üìä\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # The 'evaluate' function already calculates and prints the required metrics\n",
        "    # including Average Cumulative Reward, Action Distribution, Timely Interventions\n",
        "    # (proxy), Sepsis Treatment analysis, and notes on AUC/Mortality limitations.\n",
        "    # We just need to call it here.\n",
        "\n",
        "    evaluate(policy.policy, env, num_episodes=num_episodes)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"       ‚úÖ Evaluation Reporting Complete ‚úÖ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "report_evaluation_metrics(trained_policy, eval_env, num_episodes=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Blj8NlHm6K6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
